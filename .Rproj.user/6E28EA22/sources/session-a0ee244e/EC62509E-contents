library(tidyverse)
library(wordVectors)
library(vroom)
library(pbapply)
library(readxl)
library(ggrepel)
library(cluster)

#con <-url("https://osf.io/download/wxe2r/")
# model <- readRDS(con)
model <- readRDS("/Users/stefanhartmann/sciebo/Projekte/snowclones/word2vec/model.Rds")

# model = train_word2vec("cookbooks.txt","cookbook_vectors.bin",vectors=100,threads=4,window=3,iter=2,negative_samples=2)
# str(model)

# check results
model %>% closest_to("fish")
model %>% closest_to("woman")

# cosine distances
# Inspect the similarity of several academic disciplines by hand.
subjects = model[[c("history","literature","biology","math","stats"),average=FALSE]]
similarities = cosineSimilarity(subjects,subjects)

# all lemmas in arch-

# read nouns
arch_nouns <- read_xlsx("COW16 nouns cleaned.xlsx")

# read adj
arch_adj   <- read_xlsx("COW16 adjectives cleaned.xlsx")

# export distances for all words in arch ------

# remove false hits
arch_adj <- filter(arch_adj, Keep=="y")
arch_nouns <- filter(arch_nouns, Keep=="y")

# get bases
arch_adj$Base_Lemma <- gsub("^arch-?", "", arch_adj$Token)
arch_nouns$Base_Lemma <- gsub("^arch-?", "", arch_nouns$Token)

# make sheet for annotating lexemes / correcting lemmas
# rbind(mutate(arch_adj, pos = "ADJ"), mutate(arch_nouns, pos = "N")) %>% writexl::write_xlsx("arch_adj_n.xlsx")

# read in again
arch <- read_xlsx("arch_adj_n.xlsx")

# get lemmas
lemmas <- sort(unique(arch$Base_Lemma))

# get Cosine distance
cosDist_mds <- cosineDist(model[[tolower(lemmas), average = FALSE]], model[[tolower(lemmas), average = FALSE]]) %>% cmdscale()

# get k-means clusters
plot(1:10, sapply(1:10, function(x) kmeans(cosDist_mds, x, nstart = 25)$tot.withinss), type = "b", ylab = "WCSS")
k_means_clusters <- kmeans(cosDist_mds, 3, nstart = 25)

# as df
cosDist_mds <- cosDist_mds %>% as.data.frame() %>% rownames_to_column()
colnames(cosDist_mds)[1] <- "lemma"

# add frequency information
freqs <- rbind(arch_adj, arch_nouns) %>% select(Base_Lemma, Frequency) %>%
  group_by(Base_Lemma) %>%
  summarise(
    n = sum(Frequency)
  ) %>% setNames(c("lemma", "n"))

# freqs <- c(arch_adj$Base_Lemma, arch_nouns$Base_Lemma) %>% table %>% sort %>% as_tibble() %>% setNames(c("lemma", "n"))
cosDist_mds <- left_join(cosDist_mds, freqs) %>% replace_na(list(n = 0))
cosDist_mds$logFreq <- log1p(cosDist_mds$n)



# get PAM clusters
pams <- c()
for(i in 1:15) {
  cur_pam <- pam(cosDist_mds, k = i)
  pams[i] <- cur_pam$silinfo$avg.width
}

plot(1:15, pams, type = "b") # line elbows at ~ 3

pam_clusters <- pam(cosDist_mds, k = 3)
cosDist_mds$clusters <- factor(pam_clusters$clustering)

# add k-means clusters
cosDist_mds$kcluster <- as.factor(as.numeric(k_means_clusters$cluster))

# add a column that contains the frequencies but sets
# all frequemcies 

# get plot
set.seed(1705)
cosDist_mds %>% filter(logFreq>2) %>% ggplot(aes(x = V1, y = V2, 
                            size = n*5, label = lemma, col = clusters)) +
  geom_text_repel(max.overlaps = 25) + theme_bw() + guides(size = 'none') +
  scale_color_viridis_d(begin = .1, end = .9)  +
  guides(col = "none")
  
# ggsave("archdistances_pam.png")

# or with k-means clusters instead
set.seed(1705)
cosDist_mds %>% filter(logFreq>2) %>% ggplot(aes(x = V1, y = V2, 
                                                 size = n*5, label = lemma)) +
  stat_ellipse(aes(group = as.factor(kcluster), fill = as.factor(kcluster)),
               geom = "polygon", alpha = 0.1, color = NA) +
  geom_text_repel(max.overlaps = 25) + theme_bw() + guides(size = 'none') +
  scale_color_grey(start = .1, end = .6) +
  #scale_color_viridis_d(begin = .3, end = .8)  +
  guides(col = "none", fill = "none")

ggsave("archdistances_k_means.png", width = 7, height = 5, dpi = 600)

# clusters

cosDist_mds %>% arrange(clusters, desc(n)) %>% filter(n > 5)
